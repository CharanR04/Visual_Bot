{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['embeddings.mask_token']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from src.Model import Model\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.Tools import generate_text\n",
    "from transformers import AlbertTokenizer,ViTImageProcessor\n",
    "from src.Dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('Screenshot (3).png').convert('RGB')\n",
    "text = \"What is there in this image?\"\n",
    "#txt_out = model.encode_text(text)\n",
    "\n",
    "#merged_out = model.merge(img_out,txt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', cache_dir='Cache/Transformers')\n",
    "text_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding = 'max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 128]) torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "in_seq,attention_mask = model.get_sequence(image,text_ids)\n",
    "logits = model(in_seq,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "out_seq = generate_text(model,image,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vineyards vineyardsददददददददद vineyards vineyards vineyards vineyards vineyards vineyards vineyards somehow somehow somehow somehow somehow somehow somehow somehow novo somehow novo somehow somehow somehow somehow somehow somehow somehow somehow somehow somehow somehow inspiring inspiring inspiring inspiring pluto pluto pluto pluto pluto pluto'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
