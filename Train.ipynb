{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['embeddings.mask_token']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from src.Model import Model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from src.Tools import generate_text,get_eos_embedding,update_sequence_mask,get_one_hot\n",
    "from src.Train import one_epoch_caption,one_epoch_QA,train_caption,train_QA\n",
    "from src.Dataset import QADataset,CaptionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from src.Mean import ImagesDataset\n",
    "\n",
    "version = 1\n",
    "model = Model(_model='BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing cached mean and std\n",
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created! imgs = 23431, anns = 100575\n"
     ]
    }
   ],
   "source": [
    "QA_dataset = QADataset('Data/Annotations/QA/train.json',small=True)\n",
    "Caption_dataset = CaptionDataset('Data/Annotations/Caption/train.json',small=True)\n",
    "data_loader_cap = DataLoader(Caption_dataset, batch_size= 2, shuffle = True)\n",
    "data_loader_qa = DataLoader(QA_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator.embeddings.word_embeddings.weight: 0.0013559464132413268\n",
      "generator.embeddings.position_embeddings.weight: 0.0018837092211470008\n",
      "generator.embeddings.token_type_embeddings.weight: 0.012340676970779896\n",
      "generator.embeddings.LayerNorm.weight: 0.0006705312989652157\n",
      "generator.embeddings.LayerNorm.bias: 0.000982067664153874\n",
      "generator.encoder.layer.0.attention.self.query.weight: 0.0006871020887047052\n",
      "generator.encoder.layer.0.attention.self.query.bias: 3.980559267802164e-05\n",
      "generator.encoder.layer.0.attention.self.key.weight: 0.0008250973769463599\n",
      "generator.encoder.layer.0.attention.self.key.bias: 1.7151072229104614e-12\n",
      "generator.encoder.layer.0.attention.self.value.weight: 0.00529502471908927\n",
      "generator.encoder.layer.0.attention.self.value.bias: 0.00048423538100905716\n",
      "generator.encoder.layer.0.attention.output.dense.weight: 0.005018061492592096\n",
      "generator.encoder.layer.0.attention.output.dense.bias: 0.0006170115084387362\n",
      "generator.encoder.layer.0.attention.output.LayerNorm.weight: 0.0003567351959645748\n",
      "generator.encoder.layer.0.attention.output.LayerNorm.bias: 0.0005473697674460709\n",
      "generator.encoder.layer.0.intermediate.dense.weight: 0.0042684790678322315\n",
      "generator.encoder.layer.0.intermediate.dense.bias: 0.00019804132170975208\n",
      "generator.encoder.layer.0.output.dense.weight: 0.0026843573432415724\n",
      "generator.encoder.layer.0.output.dense.bias: 0.0005490492912940681\n",
      "generator.encoder.layer.0.output.LayerNorm.weight: 0.0008499814430251718\n",
      "generator.encoder.layer.0.output.LayerNorm.bias: 0.0007691084174439311\n",
      "generator.encoder.layer.1.attention.self.query.weight: 0.0009742025285959244\n",
      "generator.encoder.layer.1.attention.self.query.bias: 5.1581366278696805e-05\n",
      "generator.encoder.layer.1.attention.self.key.weight: 0.0009876887779682875\n",
      "generator.encoder.layer.1.attention.self.key.bias: 2.616668817387313e-12\n",
      "generator.encoder.layer.1.attention.self.value.weight: 0.005864561069756746\n",
      "generator.encoder.layer.1.attention.self.value.bias: 0.0004789455852005631\n",
      "generator.encoder.layer.1.attention.output.dense.weight: 0.006095217075198889\n",
      "generator.encoder.layer.1.attention.output.dense.bias: 0.0006223720265552402\n",
      "generator.encoder.layer.1.attention.output.LayerNorm.weight: 0.00039604640915058553\n",
      "generator.encoder.layer.1.attention.output.LayerNorm.bias: 0.0005897301016375422\n",
      "generator.encoder.layer.1.intermediate.dense.weight: 0.0057376110926270485\n",
      "generator.encoder.layer.1.intermediate.dense.bias: 0.00023861661611590534\n",
      "generator.encoder.layer.1.output.dense.weight: 0.0036362498067319393\n",
      "generator.encoder.layer.1.output.dense.bias: 0.0005989524652250111\n",
      "generator.encoder.layer.1.output.LayerNorm.weight: 0.0004641154664568603\n",
      "generator.encoder.layer.1.output.LayerNorm.bias: 0.0006872682133689523\n",
      "generator.encoder.layer.2.attention.self.query.weight: 0.0008413382456637919\n",
      "generator.encoder.layer.2.attention.self.query.bias: 4.5121840230422094e-05\n",
      "generator.encoder.layer.2.attention.self.key.weight: 0.0008294779108837247\n",
      "generator.encoder.layer.2.attention.self.key.bias: 2.501343099661768e-12\n",
      "generator.encoder.layer.2.attention.self.value.weight: 0.00703367218375206\n",
      "generator.encoder.layer.2.attention.self.value.bias: 0.0004803140473086387\n",
      "generator.encoder.layer.2.attention.output.dense.weight: 0.006788682658225298\n",
      "generator.encoder.layer.2.attention.output.dense.bias: 0.0006377022946253419\n",
      "generator.encoder.layer.2.attention.output.LayerNorm.weight: 0.00042244017822667956\n",
      "generator.encoder.layer.2.attention.output.LayerNorm.bias: 0.0006367777241393924\n",
      "generator.encoder.layer.2.intermediate.dense.weight: 0.004827912896871567\n",
      "generator.encoder.layer.2.intermediate.dense.bias: 0.00020728916570078582\n",
      "generator.encoder.layer.2.output.dense.weight: 0.0029840117786079645\n",
      "generator.encoder.layer.2.output.dense.bias: 0.0006478999275714159\n",
      "generator.encoder.layer.2.output.LayerNorm.weight: 0.0005098794936202466\n",
      "generator.encoder.layer.2.output.LayerNorm.bias: 0.0007807360962033272\n",
      "generator.encoder.layer.3.attention.self.query.weight: 0.001194872660562396\n",
      "generator.encoder.layer.3.attention.self.query.bias: 7.707477197982371e-05\n",
      "generator.encoder.layer.3.attention.self.key.weight: 0.001082449802197516\n",
      "generator.encoder.layer.3.attention.self.key.bias: 3.2045258373941676e-12\n",
      "generator.encoder.layer.3.attention.self.value.weight: 0.00972090382128954\n",
      "generator.encoder.layer.3.attention.self.value.bias: 0.0006757538649253547\n",
      "generator.encoder.layer.3.attention.output.dense.weight: 0.007981715723872185\n",
      "generator.encoder.layer.3.attention.output.dense.bias: 0.0007465190137736499\n",
      "generator.encoder.layer.3.attention.output.LayerNorm.weight: 0.0005362436058931053\n",
      "generator.encoder.layer.3.attention.output.LayerNorm.bias: 0.0007707905606366694\n",
      "generator.encoder.layer.3.intermediate.dense.weight: 0.005903938319534063\n",
      "generator.encoder.layer.3.intermediate.dense.bias: 0.0002907074522227049\n",
      "generator.encoder.layer.3.output.dense.weight: 0.005741563625633717\n",
      "generator.encoder.layer.3.output.dense.bias: 0.0007719986024312675\n",
      "generator.encoder.layer.3.output.LayerNorm.weight: 0.0007263642037287354\n",
      "generator.encoder.layer.3.output.LayerNorm.bias: 0.0009615880553610623\n",
      "generator.encoder.layer.4.attention.self.query.weight: 0.000748771708458662\n",
      "generator.encoder.layer.4.attention.self.query.bias: 4.592150071403012e-05\n",
      "generator.encoder.layer.4.attention.self.key.weight: 0.0009082822944037616\n",
      "generator.encoder.layer.4.attention.self.key.bias: 1.9981932775081646e-12\n",
      "generator.encoder.layer.4.attention.self.value.weight: 0.010832561179995537\n",
      "generator.encoder.layer.4.attention.self.value.bias: 0.0006801300914958119\n",
      "generator.encoder.layer.4.attention.output.dense.weight: 0.01083416398614645\n",
      "generator.encoder.layer.4.attention.output.dense.bias: 0.0006884261383675039\n",
      "generator.encoder.layer.4.attention.output.LayerNorm.weight: 0.000753304862882942\n",
      "generator.encoder.layer.4.attention.output.LayerNorm.bias: 0.0009098501759581268\n",
      "generator.encoder.layer.4.intermediate.dense.weight: 0.007503055036067963\n",
      "generator.encoder.layer.4.intermediate.dense.bias: 0.00034520827466621995\n",
      "generator.encoder.layer.4.output.dense.weight: 0.01056180614978075\n",
      "generator.encoder.layer.4.output.dense.bias: 0.000903262582141906\n",
      "generator.encoder.layer.4.output.LayerNorm.weight: 0.0008628922514617443\n",
      "generator.encoder.layer.4.output.LayerNorm.bias: 0.0011114305816590786\n",
      "generator.encoder.layer.5.attention.self.query.weight: 0.00047673139488324523\n",
      "generator.encoder.layer.5.attention.self.query.bias: 2.500212394807022e-05\n",
      "generator.encoder.layer.5.attention.self.key.weight: 0.0005725326482206583\n",
      "generator.encoder.layer.5.attention.self.key.bias: 2.0757443073421422e-12\n",
      "generator.encoder.layer.5.attention.self.value.weight: 0.011314105242490768\n",
      "generator.encoder.layer.5.attention.self.value.bias: 0.0006048253271728754\n",
      "generator.encoder.layer.5.attention.output.dense.weight: 0.014366153627634048\n",
      "generator.encoder.layer.5.attention.output.dense.bias: 0.0008534931694157422\n",
      "generator.encoder.layer.5.attention.output.LayerNorm.weight: 0.0008444058476015925\n",
      "generator.encoder.layer.5.attention.output.LayerNorm.bias: 0.0010815138230100274\n",
      "generator.encoder.layer.5.intermediate.dense.weight: 0.006448822561651468\n",
      "generator.encoder.layer.5.intermediate.dense.bias: 0.0002873471239581704\n",
      "generator.encoder.layer.5.output.dense.weight: 0.0058865854516625404\n",
      "generator.encoder.layer.5.output.dense.bias: 0.0010621948167681694\n",
      "generator.encoder.layer.5.output.LayerNorm.weight: 0.0008939039544202387\n",
      "generator.encoder.layer.5.output.LayerNorm.bias: 0.0011618869611993432\n",
      "generator.encoder.layer.6.attention.self.query.weight: 0.000457673886558041\n",
      "generator.encoder.layer.6.attention.self.query.bias: 2.3606489776284434e-05\n",
      "generator.encoder.layer.6.attention.self.key.weight: 0.00043003499740734696\n",
      "generator.encoder.layer.6.attention.self.key.bias: 2.2481717008859814e-12\n",
      "generator.encoder.layer.6.attention.self.value.weight: 0.011935286223888397\n",
      "generator.encoder.layer.6.attention.self.value.bias: 0.0006312384502962232\n",
      "generator.encoder.layer.6.attention.output.dense.weight: 0.0141147430986166\n",
      "generator.encoder.layer.6.attention.output.dense.bias: 0.0008869536686688662\n",
      "generator.encoder.layer.6.attention.output.LayerNorm.weight: 0.0008873915066942573\n",
      "generator.encoder.layer.6.attention.output.LayerNorm.bias: 0.0010801982134580612\n",
      "generator.encoder.layer.6.intermediate.dense.weight: 0.006663934793323278\n",
      "generator.encoder.layer.6.intermediate.dense.bias: 0.00030417137895710766\n",
      "generator.encoder.layer.6.output.dense.weight: 0.005296236369758844\n",
      "generator.encoder.layer.6.output.dense.bias: 0.0010756368283182383\n",
      "generator.encoder.layer.6.output.LayerNorm.weight: 0.0009403515141457319\n",
      "generator.encoder.layer.6.output.LayerNorm.bias: 0.0011245282366871834\n",
      "generator.encoder.layer.7.attention.self.query.weight: 0.00019933024304918945\n",
      "generator.encoder.layer.7.attention.self.query.bias: 1.0199354619544465e-05\n",
      "generator.encoder.layer.7.attention.self.key.weight: 0.00018775810895022005\n",
      "generator.encoder.layer.7.attention.self.key.bias: 2.2366236467064038e-12\n",
      "generator.encoder.layer.7.attention.self.value.weight: 0.01121975015848875\n",
      "generator.encoder.layer.7.attention.self.value.bias: 0.0005902305128984153\n",
      "generator.encoder.layer.7.attention.output.dense.weight: 0.011735877953469753\n",
      "generator.encoder.layer.7.attention.output.dense.bias: 0.0008554749074392021\n",
      "generator.encoder.layer.7.attention.output.LayerNorm.weight: 0.0008989424677565694\n",
      "generator.encoder.layer.7.attention.output.LayerNorm.bias: 0.0010700755519792438\n",
      "generator.encoder.layer.7.intermediate.dense.weight: 0.008285785093903542\n",
      "generator.encoder.layer.7.intermediate.dense.bias: 0.00037194552714936435\n",
      "generator.encoder.layer.7.output.dense.weight: 0.0060803997330367565\n",
      "generator.encoder.layer.7.output.dense.bias: 0.001028571859933436\n",
      "generator.encoder.layer.7.output.LayerNorm.weight: 0.0008841407252475619\n",
      "generator.encoder.layer.7.output.LayerNorm.bias: 0.0011220175074413419\n",
      "generator.encoder.layer.8.attention.self.query.weight: 0.00035937532084062696\n",
      "generator.encoder.layer.8.attention.self.query.bias: 1.896975481940899e-05\n",
      "generator.encoder.layer.8.attention.self.key.weight: 0.0003485125198494643\n",
      "generator.encoder.layer.8.attention.self.key.bias: 2.678162162525477e-12\n",
      "generator.encoder.layer.8.attention.self.value.weight: 0.013085846789181232\n",
      "generator.encoder.layer.8.attention.self.value.bias: 0.0007129578734748065\n",
      "generator.encoder.layer.8.attention.output.dense.weight: 0.012731336988508701\n",
      "generator.encoder.layer.8.attention.output.dense.bias: 0.0008832539315335453\n",
      "generator.encoder.layer.8.attention.output.LayerNorm.weight: 0.0007924287347123027\n",
      "generator.encoder.layer.8.attention.output.LayerNorm.bias: 0.0010083392262458801\n",
      "generator.encoder.layer.8.intermediate.dense.weight: 0.008491480723023415\n",
      "generator.encoder.layer.8.intermediate.dense.bias: 0.0003728352894540876\n",
      "generator.encoder.layer.8.output.dense.weight: 0.0056462339125573635\n",
      "generator.encoder.layer.8.output.dense.bias: 0.0009373006178066134\n",
      "generator.encoder.layer.8.output.LayerNorm.weight: 0.0007958480855450034\n",
      "generator.encoder.layer.8.output.LayerNorm.bias: 0.0010162872495129704\n",
      "generator.encoder.layer.9.attention.self.query.weight: 0.0003510085807647556\n",
      "generator.encoder.layer.9.attention.self.query.bias: 1.7693369954940863e-05\n",
      "generator.encoder.layer.9.attention.self.key.weight: 0.0003867646446451545\n",
      "generator.encoder.layer.9.attention.self.key.bias: 1.5334003598127532e-12\n",
      "generator.encoder.layer.9.attention.self.value.weight: 0.011058847419917583\n",
      "generator.encoder.layer.9.attention.self.value.bias: 0.0005714891594834626\n",
      "generator.encoder.layer.9.attention.output.dense.weight: 0.009774423204362392\n",
      "generator.encoder.layer.9.attention.output.dense.bias: 0.0006811231141909957\n",
      "generator.encoder.layer.9.attention.output.LayerNorm.weight: 0.0006950261304154992\n",
      "generator.encoder.layer.9.attention.output.LayerNorm.bias: 0.0008387169218622148\n",
      "generator.encoder.layer.9.intermediate.dense.weight: 0.006409894675016403\n",
      "generator.encoder.layer.9.intermediate.dense.bias: 0.00030365370912477374\n",
      "generator.encoder.layer.9.output.dense.weight: 0.004889222793281078\n",
      "generator.encoder.layer.9.output.dense.bias: 0.0008088441099971533\n",
      "generator.encoder.layer.9.output.LayerNorm.weight: 0.0007270018686540425\n",
      "generator.encoder.layer.9.output.LayerNorm.bias: 0.0008660443709231913\n",
      "generator.encoder.layer.10.attention.self.query.weight: 0.0002573812671471387\n",
      "generator.encoder.layer.10.attention.self.query.bias: 1.253626305697253e-05\n",
      "generator.encoder.layer.10.attention.self.key.weight: 0.00026630377396941185\n",
      "generator.encoder.layer.10.attention.self.key.bias: 1.5676570284950397e-12\n",
      "generator.encoder.layer.10.attention.self.value.weight: 0.010837739333510399\n",
      "generator.encoder.layer.10.attention.self.value.bias: 0.0005315385642461479\n",
      "generator.encoder.layer.10.attention.output.dense.weight: 0.008645754307508469\n",
      "generator.encoder.layer.10.attention.output.dense.bias: 0.0005637697177007794\n",
      "generator.encoder.layer.10.attention.output.LayerNorm.weight: 0.0006061947206035256\n",
      "generator.encoder.layer.10.attention.output.LayerNorm.bias: 0.0007070512510836124\n",
      "generator.encoder.layer.10.intermediate.dense.weight: 0.006218507885932922\n",
      "generator.encoder.layer.10.intermediate.dense.bias: 0.0002845067938324064\n",
      "generator.encoder.layer.10.output.dense.weight: 0.0051452587358653545\n",
      "generator.encoder.layer.10.output.dense.bias: 0.0006690121372230351\n",
      "generator.encoder.layer.10.output.LayerNorm.weight: 0.0006354258512146771\n",
      "generator.encoder.layer.10.output.LayerNorm.bias: 0.0007329454529099166\n",
      "generator.encoder.layer.11.attention.self.query.weight: 0.00030469437479041517\n",
      "generator.encoder.layer.11.attention.self.query.bias: 1.387045813316945e-05\n",
      "generator.encoder.layer.11.attention.self.key.weight: 0.0003198878257535398\n",
      "generator.encoder.layer.11.attention.self.key.bias: 3.050172094065662e-12\n",
      "generator.encoder.layer.11.attention.self.value.weight: 0.011109201237559319\n",
      "generator.encoder.layer.11.attention.self.value.bias: 0.0005107272882014513\n",
      "generator.encoder.layer.11.attention.output.dense.weight: 0.009428758174180984\n",
      "generator.encoder.layer.11.attention.output.dense.bias: 0.0004915746976621449\n",
      "generator.encoder.layer.11.attention.output.LayerNorm.weight: 0.0006341866101138294\n",
      "generator.encoder.layer.11.attention.output.LayerNorm.bias: 0.000705784244928509\n",
      "generator.encoder.layer.11.intermediate.dense.weight: 0.008219819515943527\n",
      "generator.encoder.layer.11.intermediate.dense.bias: 0.00031903255148790777\n",
      "generator.encoder.layer.11.output.dense.weight: 0.005564356688410044\n",
      "generator.encoder.layer.11.output.dense.bias: 0.0006437926203943789\n",
      "generator.encoder.layer.11.output.LayerNorm.weight: 0.0008694967837072909\n",
      "generator.encoder.layer.11.output.LayerNorm.bias: 0.001022933516651392\n",
      "pipeline.weight: 0.019158093258738518\n",
      "pipeline.bias: 0.00665659224614501\n",
      "classifier.weight: 0.025562871247529984\n",
      "classifier.bias: 0.0017575894016772509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████▌            | 1/2 [00:32<00:32, 32.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#loss_c = train_caption(20,model,data_loader_cap,optimizer,version=version)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m loss_q \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_QA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_loader_qa\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Charan\\Visual_Bot\\src\\Train.py:107\u001b[0m, in \u001b[0;36mtrain_QA\u001b[1;34m(epoch, model, dataloader, optimizer, version)\u001b[0m\n\u001b[0;32m    102\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(probablities,target)\n\u001b[0;32m    105\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m--> 107\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\torch-cuda121\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\torch-cuda121\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\torch-cuda121\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#loss_c = train_caption(20,model,data_loader_cap,optimizer,version=version)\n",
    "loss_q = train_QA(20,model,data_loader_qa,optimizer,version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6333e-22, device='cuda:0')\n",
      "tensor(7.6920e-21, device='cuda:0')\n",
      "tensor(2.6131e-21, device='cuda:0')\n",
      "tensor(3.5901e-21, device='cuda:0')\n",
      "tensor(3.1773e-21, device='cuda:0')\n",
      "tensor(3.2438e-21, device='cuda:0')\n",
      "tensor(9.9041e-23, device='cuda:0')\n",
      "tensor(3.6790e-20, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(6.1624e-22, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(2.9312e-20, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(4.2745e-21, device='cuda:0')\n",
      "tensor(9.8899e-22, device='cuda:0')\n",
      "tensor(2.9144e-20, device='cuda:0')\n",
      "tensor(3.5905e-22, device='cuda:0')\n",
      "tensor(1.7250e-19, device='cuda:0')\n",
      "tensor(2.2770e-22, device='cuda:0')\n",
      "tensor(3.0343e-19, device='cuda:0')\n",
      "tensor(2.3462e-20, device='cuda:0')\n",
      "tensor(2.5228e-19, device='cuda:0')\n",
      "tensor(1.2804e-20, device='cuda:0')\n",
      "tensor(1.7864e-18, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(3.9656e-20, device='cuda:0')\n",
      "tensor(1.9372e-21, device='cuda:0')\n",
      "tensor(4.3382e-19, device='cuda:0')\n",
      "tensor(1.4469e-21, device='cuda:0')\n",
      "tensor(1.6239e-19, device='cuda:0')\n",
      "tensor(1.2364e-20, device='cuda:0')\n",
      "tensor(3.7238e-19, device='cuda:0')\n",
      "tensor(5.3406e-21, device='cuda:0')\n",
      "tensor(1.4959e-18, device='cuda:0')\n",
      "tensor(2.6919e-21, device='cuda:0')\n",
      "tensor(1.9905e-18, device='cuda:0')\n",
      "tensor(8.8029e-20, device='cuda:0')\n",
      "tensor(8.8807e-19, device='cuda:0')\n",
      "tensor(3.8750e-20, device='cuda:0')\n",
      "tensor(8.3961e-18, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(5.1220e-19, device='cuda:0')\n",
      "tensor(2.2194e-20, device='cuda:0')\n",
      "tensor(1.2322e-18, device='cuda:0')\n",
      "tensor(2.4248e-21, device='cuda:0')\n",
      "tensor(2.1704e-19, device='cuda:0')\n",
      "tensor(5.6968e-20, device='cuda:0')\n",
      "tensor(3.4347e-19, device='cuda:0')\n",
      "tensor(7.3120e-21, device='cuda:0')\n",
      "tensor(4.9176e-19, device='cuda:0')\n",
      "tensor(1.7036e-21, device='cuda:0')\n",
      "tensor(4.2699e-19, device='cuda:0')\n",
      "tensor(8.4148e-20, device='cuda:0')\n",
      "tensor(5.7672e-19, device='cuda:0')\n",
      "tensor(2.3672e-20, device='cuda:0')\n",
      "tensor(2.3051e-18, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(6.4275e-19, device='cuda:0')\n",
      "tensor(2.7033e-20, device='cuda:0')\n",
      "tensor(2.6182e-18, device='cuda:0')\n",
      "tensor(7.8108e-21, device='cuda:0')\n",
      "tensor(1.1097e-18, device='cuda:0')\n",
      "tensor(1.2496e-19, device='cuda:0')\n",
      "tensor(1.8382e-18, device='cuda:0')\n",
      "tensor(3.3222e-20, device='cuda:0')\n",
      "tensor(5.0651e-18, device='cuda:0')\n",
      "tensor(1.4332e-20, device='cuda:0')\n",
      "tensor(7.8150e-18, device='cuda:0')\n",
      "tensor(8.7538e-19, device='cuda:0')\n",
      "tensor(4.2496e-18, device='cuda:0')\n",
      "tensor(1.8469e-19, device='cuda:0')\n",
      "tensor(5.1281e-17, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(4.3876e-18, device='cuda:0')\n",
      "tensor(1.9247e-19, device='cuda:0')\n",
      "tensor(9.7343e-18, device='cuda:0')\n",
      "tensor(2.9387e-20, device='cuda:0')\n",
      "tensor(6.2914e-18, device='cuda:0')\n",
      "tensor(4.0019e-18, device='cuda:0')\n",
      "tensor(2.2258e-17, device='cuda:0')\n",
      "tensor(5.1435e-19, device='cuda:0')\n",
      "tensor(2.8510e-17, device='cuda:0')\n",
      "tensor(8.6266e-20, device='cuda:0')\n",
      "tensor(3.0846e-17, device='cuda:0')\n",
      "tensor(7.3423e-18, device='cuda:0')\n",
      "tensor(2.6326e-17, device='cuda:0')\n",
      "tensor(1.0715e-18, device='cuda:0')\n",
      "tensor(1.5374e-16, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1.8682e-17, device='cuda:0')\n",
      "tensor(8.1095e-19, device='cuda:0')\n",
      "tensor(2.0572e-17, device='cuda:0')\n",
      "tensor(7.3289e-20, device='cuda:0')\n",
      "tensor(5.8446e-18, device='cuda:0')\n",
      "tensor(1.0777e-18, device='cuda:0')\n",
      "tensor(1.8734e-17, device='cuda:0')\n",
      "tensor(2.7037e-19, device='cuda:0')\n",
      "tensor(3.4650e-17, device='cuda:0')\n",
      "tensor(7.3549e-20, device='cuda:0')\n",
      "tensor(4.6797e-17, device='cuda:0')\n",
      "tensor(5.6785e-18, device='cuda:0')\n",
      "tensor(1.9908e-17, device='cuda:0')\n",
      "tensor(8.7123e-19, device='cuda:0')\n",
      "tensor(1.4443e-16, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(3.8532e-17, device='cuda:0')\n",
      "tensor(1.7778e-18, device='cuda:0')\n",
      "tensor(1.3934e-16, device='cuda:0')\n",
      "tensor(6.1153e-19, device='cuda:0')\n",
      "tensor(1.0390e-16, device='cuda:0')\n",
      "tensor(2.8878e-17, device='cuda:0')\n",
      "tensor(1.8416e-16, device='cuda:0')\n",
      "tensor(5.5677e-18, device='cuda:0')\n",
      "tensor(3.7316e-16, device='cuda:0')\n",
      "tensor(1.1049e-18, device='cuda:0')\n",
      "tensor(5.4686e-16, device='cuda:0')\n",
      "tensor(2.1692e-17, device='cuda:0')\n",
      "tensor(1.0825e-16, device='cuda:0')\n",
      "tensor(4.7973e-18, device='cuda:0')\n",
      "tensor(1.7833e-15, device='cuda:0')\n",
      "tensor(8.3705e-23, device='cuda:0')\n",
      "tensor(6.9859e-17, device='cuda:0')\n",
      "tensor(3.1677e-18, device='cuda:0')\n",
      "tensor(2.8260e-16, device='cuda:0')\n",
      "tensor(3.8257e-19, device='cuda:0')\n",
      "tensor(1.1911e-16, device='cuda:0')\n",
      "tensor(1.4858e-17, device='cuda:0')\n",
      "tensor(1.5401e-16, device='cuda:0')\n",
      "tensor(2.1001e-18, device='cuda:0')\n",
      "tensor(4.0514e-17, device='cuda:0')\n",
      "tensor(2.9398e-19, device='cuda:0')\n",
      "tensor(2.3704e-17, device='cuda:0')\n",
      "tensor(2.6737e-18, device='cuda:0')\n",
      "tensor(6.1387e-18, device='cuda:0')\n",
      "tensor(2.7915e-19, device='cuda:0')\n",
      "tensor(7.7635e-17, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(2.5476e-17, device='cuda:0')\n",
      "tensor(1.1950e-18, device='cuda:0')\n",
      "tensor(3.5698e-17, device='cuda:0')\n",
      "tensor(9.3303e-20, device='cuda:0')\n",
      "tensor(1.0404e-17, device='cuda:0')\n",
      "tensor(7.0924e-18, device='cuda:0')\n",
      "tensor(3.2520e-17, device='cuda:0')\n",
      "tensor(8.8579e-19, device='cuda:0')\n",
      "tensor(8.2578e-17, device='cuda:0')\n",
      "tensor(1.5296e-19, device='cuda:0')\n",
      "tensor(1.2325e-16, device='cuda:0')\n",
      "tensor(4.8821e-17, device='cuda:0')\n",
      "tensor(4.3017e-17, device='cuda:0')\n",
      "tensor(1.8433e-18, device='cuda:0')\n",
      "tensor(5.3332e-16, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(8.4823e-17, device='cuda:0')\n",
      "tensor(3.8049e-18, device='cuda:0')\n",
      "tensor(2.1918e-16, device='cuda:0')\n",
      "tensor(3.4040e-19, device='cuda:0')\n",
      "tensor(1.6694e-16, device='cuda:0')\n",
      "tensor(1.8310e-16, device='cuda:0')\n",
      "tensor(9.1932e-16, device='cuda:0')\n",
      "tensor(4.1879e-17, device='cuda:0')\n",
      "tensor(2.9586e-15, device='cuda:0')\n",
      "tensor(3.1458e-18, device='cuda:0')\n",
      "tensor(3.9336e-15, device='cuda:0')\n",
      "tensor(4.4141e-15, device='cuda:0')\n",
      "tensor(5.1729e-16, device='cuda:0')\n",
      "tensor(2.4971e-17, device='cuda:0')\n",
      "tensor(3.4245e-15, device='cuda:0')\n",
      "tensor(2.6733e-22, device='cuda:0')\n",
      "tensor(5.4656e-15, device='cuda:0')\n",
      "tensor(2.8382e-16, device='cuda:0')\n",
      "tensor(1.7822e-14, device='cuda:0')\n",
      "tensor(8.9844e-17, device='cuda:0')\n",
      "tensor(1.2964e-14, device='cuda:0')\n",
      "tensor(1.4453e-14, device='cuda:0')\n",
      "tensor(1.0246e-13, device='cuda:0')\n",
      "tensor(4.4812e-15, device='cuda:0')\n",
      "tensor(5.8519e-13, device='cuda:0')\n",
      "tensor(9.6647e-16, device='cuda:0')\n",
      "tensor(1.2645e-12, device='cuda:0')\n",
      "tensor(1.3750e-12, device='cuda:0')\n",
      "tensor(1.0893e-13, device='cuda:0')\n",
      "tensor(5.1070e-15, device='cuda:0')\n",
      "tensor(4.0248e-13, device='cuda:0')\n",
      "tensor(7.1056e-20, device='cuda:0')\n",
      "tensor(4.2193e-12, device='cuda:0')\n",
      "tensor(2.0462e-13, device='cuda:0')\n",
      "tensor(3.2624e-12, device='cuda:0')\n",
      "tensor(2.2654e-14, device='cuda:0')\n",
      "tensor(1.0368e-12, device='cuda:0')\n",
      "tensor(1.1516e-12, device='cuda:0')\n",
      "tensor(3.8553e-12, device='cuda:0')\n",
      "tensor(1.5927e-13, device='cuda:0')\n",
      "tensor(1.8432e-11, device='cuda:0')\n",
      "tensor(5.2478e-14, device='cuda:0')\n",
      "tensor(9.7592e-11, device='cuda:0')\n",
      "tensor(9.8229e-11, device='cuda:0')\n",
      "tensor(9.3469e-21, device='cuda:0')\n",
      "tensor(1.6062e-21, device='cuda:0')\n",
      "tensor(6.7588e-10, device='cuda:0')\n",
      "tensor(3.9238e-11, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        print(param.grad.norm())\n",
    "    else:\n",
    "        #print(name, None)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
